{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all the data tables together for analysis and predictive modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required packages\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import earthpy as et\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#read in the city info table\n",
    "os.chdir(os.path.join(et.io.HOME, 'Documents', 'Python_Scripts', 'PROJECT'))\n",
    "CITY_COUNTRY_lat_lon = pd.read_excel('CITY_COUNTRY_lat_lon.xlsx', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove cities with area less than 10km2\n",
    "CITY_COUNTRY_lat_lon = CITY_COUNTRY_lat_lon[CITY_COUNTRY_lat_lon['Area_2002']>9].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqua only \n",
    "#create empty dataframe to append info into \n",
    "all_monthly_data_df = pd.DataFrame(data = None)\n",
    "\n",
    "for i in range(len(CITY_COUNTRY_lat_lon)):\n",
    "    CITY_COUNTRY = CITY_COUNTRY_lat_lon.CITY_COUNTRY[i]\n",
    "    city_name = CITY_COUNTRY_lat_lon.City[i]\n",
    "    #koppen_2_letter = CITY_COUNTRY_lat_lon.Koppen_Geiger_2_Letter[i]\n",
    "\n",
    "    #read in the SUHI parameters  \n",
    "    os.chdir(os.path.join('D:\\\\','MODIS_8_day_LST', CITY_COUNTRY, 'MYD11A2'))    \n",
    "    pickle_name = 'Parameters_Day_{}_{}.pkl'.format(CITY_COUNTRY, 'MYD11A2')\n",
    "    with open(pickle_name, 'rb') as f:\n",
    "        aqua_parameters_day = pickle.load(f)\n",
    "    pickle_name = 'Parameters_Night_{}_{}.pkl'.format(CITY_COUNTRY, 'MYD11A2')\n",
    "    with open(pickle_name, 'rb') as f:\n",
    "        aqua_parameters_night = pickle.load(f)\n",
    "\n",
    "    #read in the climate parameters\n",
    "    os.chdir(os.path.join(et.io.HOME, 'Documents', 'Python_Scripts', 'PROJECT', 'ERA5', CITY_COUNTRY))\n",
    "    pickle_name = '{}_clim_rur_urb'.format(city_name)\n",
    "    with open(pickle_name, 'rb') as f:\n",
    "        clim_df = pickle.load(f)\n",
    "    \n",
    "    clim_df2 = clim_df.reset_index()[['year', 'month', 'urban_evap_fract', 'urban_rh', 'rural_evap_fract', 'rural_rh',\n",
    "                                     'rural_tp','urban_tp','rural_t2m','urban_t2m','rural_ssr','urban_ssr']]\n",
    "    \n",
    "    # get the annual average ef\n",
    "    annual_ef_df = clim_df2.groupby(['year'], as_index=False).mean()[['year','urban_evap_fract','rural_evap_fract']].rename(\n",
    "        columns={\"urban_evap_fract\": \"annual_urban_ef\", \"rural_evap_fract\": \"annual_rural_ef\"})\n",
    "\n",
    "    \n",
    "    #################################### AQUA DAY #############################    \n",
    "    #add in the grouping the the parameters dataset to get a0 and mean SUHI\n",
    "    #aqua_parameters_day\n",
    "    aqua_parameters_day['year'] = pd.to_numeric(aqua_parameters_day['Year'], downcast='integer')\n",
    "    aqua_parameters_day['month'] = pd.to_numeric(aqua_parameters_day['Month'], downcast='integer')\n",
    "    \n",
    "    group = (aqua_parameters_day.groupby(['year', 'month']).mean().reset_index()\n",
    "                          [['year','month','day_a0','method_2_SUHI','quantile_a0','footprint_area']])#'buffer_5km_SUHI',\n",
    "    \n",
    "    aqua_mon_averages = pd.merge(group, clim_df2, on =['year', 'month'], how = 'inner').drop_duplicates()\n",
    "    aqua_mon_averages = pd.merge(aqua_mon_averages, annual_ef_df, on =['year'], how = 'left')\n",
    "    \n",
    "    aqua_mon_averages['Overpass'] = '13:30'    \n",
    "    aqua_mon_averages['City'] = city_name\n",
    "    #aqua_mon_averages['Koppen_Geiger_2_Letter'] = koppen_2_letter\n",
    "    \n",
    "    #################################### AQUA NIGHT ##########################   \n",
    "    #add in the grouping the the parameters dataset to get a0 and mean SUHI\n",
    "    #aqua_parameters_night\n",
    "    aqua_parameters_night['year'] = pd.to_numeric(aqua_parameters_night['Year'], downcast='integer')\n",
    "    aqua_parameters_night['month'] = pd.to_numeric(aqua_parameters_night['Month'], downcast='integer')\n",
    "    \n",
    "    group = (aqua_parameters_night.groupby(['year', 'month']).mean().reset_index()\n",
    "                          [['year','month','night_a0','method_2_SUHI','quantile_a0','footprint_area']])#,'buffer_5km_SUHI'\n",
    "    \n",
    "    aqua_night_mon_averages = pd.merge(group, clim_df2, on =['year', 'month'], how = 'inner').drop_duplicates()\n",
    "    aqua_night_mon_averages = pd.merge(aqua_night_mon_averages, annual_ef_df, on =['year'], how = 'left')\n",
    "    \n",
    "    aqua_night_mon_averages['Overpass'] = '01:30'    \n",
    "    aqua_night_mon_averages['City'] = city_name\n",
    "    #aqua_night_mon_averages['Koppen_Geiger_2_Letter'] = koppen_2_letter    \n",
    "\n",
    "    #renames a0s to a0 so can be joined into same df\n",
    "    aqua_mon_averages = aqua_mon_averages.rename(columns={\"day_a0\": \"a0\"})   \n",
    "    aqua_night_mon_averages = aqua_night_mon_averages.rename(columns={\"night_a0\": \"a0\"}) \n",
    "\n",
    "    all_monthly_data_df = all_monthly_data_df.append(aqua_mon_averages)\n",
    "    all_monthly_data_df = all_monthly_data_df.append(aqua_night_mon_averages)\n",
    "\n",
    "##save the table as pickle\n",
    "#os.chdir(os.path.join(et.io.HOME, 'Documents', 'Python_Scripts', 'PROJECT','UHI_Project_Pickle_Files','All_cities'))   \n",
    "#with open('aqua_no_evi_monthly_data_df.pkl', 'wb') as f:\n",
    "#    pickle.dump(all_monthly_data_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PIRACICABA_BRAZIL'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CITY_COUNTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in the city area and eccentricity\n",
    "# create a df for the areas to merge\n",
    "# for years greater than 2015, use 2015 area\n",
    "\n",
    "year_list = ['2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013',\n",
    "                  '2014','2015']\n",
    "year_nans = ['2016','2017','2018','2019','2020']\n",
    "area_list = []\n",
    "city_list = []\n",
    "year_loop_list = []\n",
    "eccentricity_list = []\n",
    "\n",
    "for country_i, City in enumerate(CITY_COUNTRY_lat_lon.City):   \n",
    "    for n, year in enumerate(year_list):\n",
    "        area_list.append(CITY_COUNTRY_lat_lon['Area_{}'.format(year)][country_i])\n",
    "        eccentricity_list.append(CITY_COUNTRY_lat_lon['eccen_{}'.format(year)][country_i])\n",
    "        city_list.append(City)\n",
    "        year_loop_list.append(year)\n",
    "    for m, nan_year in enumerate(year_nans):\n",
    "        area_list.append(CITY_COUNTRY_lat_lon['Area_2015'][country_i])\n",
    "        eccentricity_list.append(CITY_COUNTRY_lat_lon['eccen_2015'.format(year)][country_i])\n",
    "        city_list.append(City)\n",
    "        year_loop_list.append(nan_year)\n",
    "        \n",
    "area_df = pd.DataFrame()\n",
    "area_df['City'] = city_list\n",
    "year_loop_list = list(map(int, year_loop_list))\n",
    "area_df['year'] = year_loop_list\n",
    "area_df['Area'] = area_list        \n",
    "area_df['Eccentricity'] = eccentricity_list\n",
    "\n",
    "# merge on\n",
    "all_monthly_data_df = pd.merge(all_monthly_data_df, area_df, on =['City','year'], how = 'left')\n",
    "\n",
    "##save the table as pickle\n",
    "os.chdir(os.path.join(et.io.HOME, 'Documents', 'Python_Scripts', 'PROJECT','UHI_Project_Pickle_Files','All_cities'))   \n",
    "with open('aqua_no_evi_monthly_data_df.pkl', 'wb') as f:\n",
    "    pickle.dump(all_monthly_data_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove 2021\n",
    "all_monthly_data_df = all_monthly_data_df[all_monthly_data_df.year != 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in the evi data\n",
    "# create a dataframe with all the EVI data\n",
    "all_evi_df = pd.DataFrame(data = None)\n",
    "\n",
    "for i in range(len(CITY_COUNTRY_lat_lon)):\n",
    "    CITY_COUNTRY = CITY_COUNTRY_lat_lon['CITY_COUNTRY'][i] \n",
    "    city_name = CITY_COUNTRY_lat_lon['City'][i]  \n",
    "    #open the city EVI df\n",
    "    os.chdir(os.path.join('D:\\\\','MODIS_NDVI', CITY_COUNTRY))\n",
    "    pickle_name = 'vi_means_df_{}.pkl'.format(CITY_COUNTRY)\n",
    "    with open(pickle_name, 'rb') as f:\n",
    "        vi_means_df = pickle.load(f)\n",
    "    #add the city name\n",
    "    vi_means_df['City'] = city_name\n",
    "    #append to overall df\n",
    "    all_evi_df = all_evi_df.append(vi_means_df)\n",
    "\n",
    "#change year/ month dtypes to int so can be matched on merge\n",
    "all_evi_df['Year'] = all_evi_df['Year'].apply(int)\n",
    "all_evi_df['Month'] = all_evi_df['Month'].apply(int)\n",
    "all_monthly_data_df['Year'] = all_monthly_data_df['year'].apply(int)\n",
    "all_monthly_data_df['Month'] = all_monthly_data_df['month'].apply(int)\n",
    "    \n",
    "#merge the datasets\n",
    "all_monthly_data_df2 = pd.merge(all_monthly_data_df, all_evi_df, how='left', left_on=['City','Year','Month'], \n",
    "                                right_on = ['City','Year','Month'])    \n",
    "      \n",
    "#drop useless columns \n",
    "all_monthly_data_df2 = all_monthly_data_df2.drop(columns=['Year', 'Month','Aqua_Filename', 'Terra_Filename',\n",
    "                                                         'Terra_Filename_top','Aqua_Filename_top'])\n",
    "\n",
    "#save the new dataset\n",
    "os.chdir(os.path.join(et.io.HOME, 'Documents', 'Python_Scripts', 'PROJECT','UHI_Project_Pickle_Files','All_cities')) \n",
    "with open('aqua_all_monthly_data_df2.pkl', 'wb') as f:\n",
    "    pickle.dump(all_monthly_data_df2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in later? 'buffer_5km_SUHI','quantile_a0','footprint_area'\n",
    "\n",
    "#create empty dataframe to append info into \n",
    "all_monthly_data_df = pd.DataFrame(data = None)\n",
    "\n",
    "for i in range(len(CITY_COUNTRY_lat_lon)):\n",
    "    CITY_COUNTRY = CITY_COUNTRY_lat_lon.CITY_COUNTRY[i]\n",
    "    city_name = CITY_COUNTRY_lat_lon.City[i]\n",
    "    koppen_2_letter = CITY_COUNTRY_lat_lon.Koppen_Geiger_2_Letter[i]\n",
    "\n",
    "    #read in the SUHI parameters\n",
    "    os.chdir(os.path.join('D:\\\\','MODIS_8_day_LST', CITY_COUNTRY, 'MOD11A2'))\n",
    "    pickle_name = 'Parameters_Day_{}_{}.pkl'.format(CITY_COUNTRY, 'MOD11A2')\n",
    "    with open(pickle_name, 'rb') as f:\n",
    "        terra_parameters_day = pickle.load(f)\n",
    "    pickle_name = 'Parameters_Night_{}_{}.pkl'.format(CITY_COUNTRY, 'MOD11A2')\n",
    "    with open(pickle_name, 'rb') as f:\n",
    "        terra_parameters_night = pickle.load(f)   \n",
    "    os.chdir(os.path.join('D:\\\\','MODIS_8_day_LST', CITY_COUNTRY, 'MYD11A2'))    \n",
    "    pickle_name = 'Parameters_Day_{}_{}.pkl'.format(CITY_COUNTRY, 'MYD11A2')\n",
    "    with open(pickle_name, 'rb') as f:\n",
    "        aqua_parameters_day = pickle.load(f)\n",
    "    pickle_name = 'Parameters_Night_{}_{}.pkl'.format(CITY_COUNTRY, 'MYD11A2')\n",
    "    with open(pickle_name, 'rb') as f:\n",
    "        aqua_parameters_night = pickle.load(f)\n",
    "\n",
    "    #read in the climate parameters\n",
    "    os.chdir(os.path.join(et.io.HOME, 'Documents', 'Python_Scripts', 'PROJECT', 'ERA5', CITY_COUNTRY))\n",
    "    pickle_name = '{}_clim_rur_urb'.format(city_name)\n",
    "    with open(pickle_name, 'rb') as f:\n",
    "        clim_df = pickle.load(f)\n",
    "    \n",
    "    clim_df2 = clim_df.reset_index()[['year', 'month', 'overall_evap_fract', 'overall_rh', \n",
    "                                      'urban_evap_fract', 'urban_rh', 'rural_evap_fract', 'rural_rh',\n",
    "                                     'overall_tp','rural_tp','urban_tp','overall_t2m','rural_t2m','urban_t2m',\n",
    "                                     'overall_ssr','rural_ssr','urban_ssr']]\n",
    "    \n",
    "    #################################### TERRA DAY ##############################    \n",
    "    #add in the grouping the the parameters dataset to get a0 and mean SUHI\n",
    "    #convert the year and month columns to integer so they are equal for the merge\n",
    "    terra_parameters_day['year'] = pd.to_numeric(terra_parameters_day['Year'], downcast='integer')\n",
    "    terra_parameters_day['month'] = pd.to_numeric(terra_parameters_day['Month'], downcast='integer')\n",
    "    \n",
    "    group = (terra_parameters_day.groupby(['year', 'month']).mean().reset_index()\n",
    "                          [['year','month','day_a0','method_2_SUHI','quantile_a0']])\n",
    "    \n",
    "    terra_mon_averages = pd.merge(group, clim_df2, on =['year', 'month'], how = 'inner').drop_duplicates()\n",
    "    \n",
    "    terra_mon_averages['Overpass'] = '10:30'    \n",
    "    terra_mon_averages['City'] = city_name\n",
    "    terra_mon_averages['Koppen_Geiger_2_Letter'] = koppen_2_letter\n",
    "    \n",
    "    #################################### AQUA DAY #############################    \n",
    "    #add in the grouping the the parameters dataset to get a0 and mean SUHI\n",
    "    #aqua_parameters_day\n",
    "    aqua_parameters_day['year'] = pd.to_numeric(aqua_parameters_day['Year'], downcast='integer')\n",
    "    aqua_parameters_day['month'] = pd.to_numeric(aqua_parameters_day['Month'], downcast='integer')\n",
    "    \n",
    "    group = (aqua_parameters_day.groupby(['year', 'month']).mean().reset_index()\n",
    "                          [['year','month','day_a0','method_2_SUHI','quantile_a0']])\n",
    "    \n",
    "    aqua_mon_averages = pd.merge(group, clim_df2, on =['year', 'month'], how = 'inner').drop_duplicates()\n",
    "    \n",
    "    aqua_mon_averages['Overpass'] = '13:30'    \n",
    "    aqua_mon_averages['City'] = city_name\n",
    "    aqua_mon_averages['Koppen_Geiger_2_Letter'] = koppen_2_letter\n",
    "    \n",
    "    ################################### TERRA NIGHT ###########################    \n",
    "    #add in the grouping the the parameters dataset to get a0 and mean SUHI\n",
    "    #terra_parameters_night\n",
    "    terra_parameters_night['year'] = pd.to_numeric(terra_parameters_night['Year'], downcast='integer')\n",
    "    terra_parameters_night['month'] = pd.to_numeric(terra_parameters_night['Month'], downcast='integer')\n",
    "    \n",
    "    group = (terra_parameters_night.groupby(['year', 'month']).mean().reset_index()\n",
    "                          [['year','month','night_a0','method_2_SUHI','quantile_a0']])\n",
    "    \n",
    "    terra_night_mon_averages = pd.merge(group, clim_df2, on =['year', 'month'], how = 'inner').drop_duplicates()\n",
    "    \n",
    "    terra_night_mon_averages['Overpass'] = '22:30'    \n",
    "    terra_night_mon_averages['City'] = city_name\n",
    "    terra_night_mon_averages['Koppen_Geiger_2_Letter'] = koppen_2_letter\n",
    "    \n",
    "    #################################### AQUA NIGHT ##########################   \n",
    "    #add in the grouping the the parameters dataset to get a0 and mean SUHI\n",
    "    #aqua_parameters_night\n",
    "    aqua_parameters_night['year'] = pd.to_numeric(aqua_parameters_night['Year'], downcast='integer')\n",
    "    aqua_parameters_night['month'] = pd.to_numeric(aqua_parameters_night['Month'], downcast='integer')\n",
    "    \n",
    "    group = (aqua_parameters_night.groupby(['year', 'month']).mean().reset_index()\n",
    "                          [['year','month','night_a0','method_2_SUHI','quantile_a0']])\n",
    "    \n",
    "    aqua_night_mon_averages = pd.merge(group, clim_df2, on =['year', 'month'], how = 'inner').drop_duplicates()\n",
    "    \n",
    "    aqua_night_mon_averages['Overpass'] = '01:30'    \n",
    "    aqua_night_mon_averages['City'] = city_name\n",
    "    aqua_night_mon_averages['Koppen_Geiger_2_Letter'] = koppen_2_letter    \n",
    "\n",
    "    #renames a0s to a0 so can be joined into same df\n",
    "    terra_mon_averages = terra_mon_averages.rename(columns={\"day_a0\": \"a0\"})\n",
    "    aqua_mon_averages = aqua_mon_averages.rename(columns={\"day_a0\": \"a0\"})\n",
    "    terra_night_mon_averages = terra_night_mon_averages.rename(columns={\"night_a0\": \"a0\"})\n",
    "    aqua_night_mon_averages = aqua_night_mon_averages.rename(columns={\"night_a0\": \"a0\"}) \n",
    "\n",
    "    all_monthly_data_df = all_monthly_data_df.append(terra_mon_averages)\n",
    "    all_monthly_data_df = all_monthly_data_df.append(aqua_mon_averages)\n",
    "    all_monthly_data_df = all_monthly_data_df.append(terra_night_mon_averages)\n",
    "    all_monthly_data_df = all_monthly_data_df.append(aqua_night_mon_averages)\n",
    "\n",
    "#save the table as pickle\n",
    "os.chdir(os.path.join(et.io.HOME, 'Documents', 'Python_Scripts', 'PROJECT','UHI_Project_Pickle_Files','All_cities'))   \n",
    "with open('all_monthly_data_df.pkl', 'wb') as f:\n",
    "    pickle.dump(all_monthly_data_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add in the evi data\n",
    "#create a dataframe with all the EVI data\n",
    "all_evi_df = pd.DataFrame(data = None)\n",
    "\n",
    "for i in range(len(CITY_COUNTRY_lat_lon)):\n",
    "    CITY_COUNTRY = CITY_COUNTRY_lat_lon['CITY_COUNTRY'][i] \n",
    "    city_name = CITY_COUNTRY_lat_lon['City'][i]  \n",
    "    #open the city EVI df\n",
    "    os.chdir(os.path.join('D:\\\\','MODIS_NDVI', CITY_COUNTRY))\n",
    "    pickle_name = 'vi_means_df_{}.pkl'.format(CITY_COUNTRY)\n",
    "    with open(pickle_name, 'rb') as f:\n",
    "        vi_means_df = pickle.load(f)\n",
    "    #add the city name\n",
    "    vi_means_df['City'] = city_name\n",
    "    #append to overall df\n",
    "    all_evi_df = all_evi_df.append(vi_means_df)\n",
    "\n",
    "#change year/ month dtypes to int so can be matched on merge\n",
    "all_evi_df['Year'] = all_evi_df['Year'].apply(int)\n",
    "all_evi_df['Month'] = all_evi_df['Month'].apply(int)\n",
    "all_monthly_data_df['Year'] = all_monthly_data_df['year'].apply(int)\n",
    "all_monthly_data_df['Month'] = all_monthly_data_df['month'].apply(int)\n",
    "    \n",
    "#merge the datasets\n",
    "all_monthly_data_df2 = pd.merge(all_monthly_data_df, all_evi_df, how='left', left_on=['City','Year','Month'], \n",
    "                                right_on = ['City','Year','Month'])    \n",
    "      \n",
    "#drop useless columns \n",
    "all_monthly_data_df2 = all_monthly_data_df2.drop(columns=['Year', 'Month','Aqua_Filename', 'Terra_Filename',\n",
    "                                                         'Terra_Filename_top','Aqua_Filename_top'])\n",
    "\n",
    "#save the new dataset\n",
    "os.chdir(os.path.join(et.io.HOME, 'Documents', 'Python_Scripts', 'PROJECT','UHI_Project_Pickle_Files','All_cities')) \n",
    "with open('all_monthly_data_df2.pkl', 'wb') as f:\n",
    "    pickle.dump(all_monthly_data_df2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>a0</th>\n",
       "      <th>method_2_SUHI</th>\n",
       "      <th>quantile_a0</th>\n",
       "      <th>overall_evap_fract</th>\n",
       "      <th>overall_rh</th>\n",
       "      <th>urban_evap_fract</th>\n",
       "      <th>urban_rh</th>\n",
       "      <th>rural_evap_fract</th>\n",
       "      <th>...</th>\n",
       "      <th>urban_t2m</th>\n",
       "      <th>overall_ssr</th>\n",
       "      <th>rural_ssr</th>\n",
       "      <th>urban_ssr</th>\n",
       "      <th>Overpass</th>\n",
       "      <th>City</th>\n",
       "      <th>Koppen_Geiger_2_Letter</th>\n",
       "      <th>rur_mean_evi</th>\n",
       "      <th>urb_mean_evi</th>\n",
       "      <th>pixel_reliability_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>2.906066</td>\n",
       "      <td>0.760998</td>\n",
       "      <td>3.650000</td>\n",
       "      <td>0.757847</td>\n",
       "      <td>95.929985</td>\n",
       "      <td>0.750305</td>\n",
       "      <td>95.864914</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>...</td>\n",
       "      <td>291.251892</td>\n",
       "      <td>183.240585</td>\n",
       "      <td>183.293046</td>\n",
       "      <td>181.423597</td>\n",
       "      <td>10:30</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>Cw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002</td>\n",
       "      <td>2</td>\n",
       "      <td>2.596784</td>\n",
       "      <td>0.801647</td>\n",
       "      <td>2.140000</td>\n",
       "      <td>0.758275</td>\n",
       "      <td>95.736839</td>\n",
       "      <td>0.752681</td>\n",
       "      <td>95.705627</td>\n",
       "      <td>0.758437</td>\n",
       "      <td>...</td>\n",
       "      <td>291.766907</td>\n",
       "      <td>191.752204</td>\n",
       "      <td>191.823445</td>\n",
       "      <td>189.284798</td>\n",
       "      <td>10:30</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>Cw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.568914</td>\n",
       "      <td>2.869500</td>\n",
       "      <td>0.753038</td>\n",
       "      <td>96.402504</td>\n",
       "      <td>0.745333</td>\n",
       "      <td>96.382164</td>\n",
       "      <td>0.753261</td>\n",
       "      <td>...</td>\n",
       "      <td>291.673859</td>\n",
       "      <td>183.512472</td>\n",
       "      <td>183.582602</td>\n",
       "      <td>181.083538</td>\n",
       "      <td>10:30</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>Cw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002</td>\n",
       "      <td>4</td>\n",
       "      <td>1.820394</td>\n",
       "      <td>0.734614</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>0.793239</td>\n",
       "      <td>93.959358</td>\n",
       "      <td>0.781038</td>\n",
       "      <td>93.962883</td>\n",
       "      <td>0.793591</td>\n",
       "      <td>...</td>\n",
       "      <td>291.809235</td>\n",
       "      <td>198.372499</td>\n",
       "      <td>198.434762</td>\n",
       "      <td>196.216036</td>\n",
       "      <td>10:30</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>Cw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002</td>\n",
       "      <td>5</td>\n",
       "      <td>1.585061</td>\n",
       "      <td>0.229486</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.719868</td>\n",
       "      <td>84.684525</td>\n",
       "      <td>0.731550</td>\n",
       "      <td>84.930412</td>\n",
       "      <td>0.719531</td>\n",
       "      <td>...</td>\n",
       "      <td>291.129089</td>\n",
       "      <td>214.940667</td>\n",
       "      <td>215.010496</td>\n",
       "      <td>212.522158</td>\n",
       "      <td>10:30</td>\n",
       "      <td>Huambo</td>\n",
       "      <td>Cw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15661</th>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>2.259935</td>\n",
       "      <td>1.288898</td>\n",
       "      <td>1.960000</td>\n",
       "      <td>0.113581</td>\n",
       "      <td>81.298790</td>\n",
       "      <td>0.101158</td>\n",
       "      <td>81.256645</td>\n",
       "      <td>0.114431</td>\n",
       "      <td>...</td>\n",
       "      <td>292.991486</td>\n",
       "      <td>211.714841</td>\n",
       "      <td>211.779518</td>\n",
       "      <td>210.769278</td>\n",
       "      <td>01:30</td>\n",
       "      <td>Bulawayo</td>\n",
       "      <td>BS</td>\n",
       "      <td>0.142561</td>\n",
       "      <td>0.143414</td>\n",
       "      <td>97.407746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15662</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>1.765050</td>\n",
       "      <td>0.766030</td>\n",
       "      <td>1.694000</td>\n",
       "      <td>0.233809</td>\n",
       "      <td>82.042282</td>\n",
       "      <td>0.232444</td>\n",
       "      <td>82.009880</td>\n",
       "      <td>0.233902</td>\n",
       "      <td>...</td>\n",
       "      <td>295.421143</td>\n",
       "      <td>249.850531</td>\n",
       "      <td>249.928315</td>\n",
       "      <td>248.713337</td>\n",
       "      <td>01:30</td>\n",
       "      <td>Bulawayo</td>\n",
       "      <td>BS</td>\n",
       "      <td>0.173692</td>\n",
       "      <td>0.176158</td>\n",
       "      <td>90.179933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15663</th>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>2.104820</td>\n",
       "      <td>0.983248</td>\n",
       "      <td>1.989333</td>\n",
       "      <td>0.346746</td>\n",
       "      <td>84.690918</td>\n",
       "      <td>0.351147</td>\n",
       "      <td>84.664955</td>\n",
       "      <td>0.346445</td>\n",
       "      <td>...</td>\n",
       "      <td>296.661865</td>\n",
       "      <td>216.163103</td>\n",
       "      <td>216.242484</td>\n",
       "      <td>215.002569</td>\n",
       "      <td>01:30</td>\n",
       "      <td>Bulawayo</td>\n",
       "      <td>BS</td>\n",
       "      <td>0.255260</td>\n",
       "      <td>0.236684</td>\n",
       "      <td>55.504727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15664</th>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>0.968285</td>\n",
       "      <td>0.238787</td>\n",
       "      <td>1.058500</td>\n",
       "      <td>0.773027</td>\n",
       "      <td>95.608086</td>\n",
       "      <td>0.787646</td>\n",
       "      <td>95.623184</td>\n",
       "      <td>0.772028</td>\n",
       "      <td>...</td>\n",
       "      <td>293.581665</td>\n",
       "      <td>172.261565</td>\n",
       "      <td>172.344445</td>\n",
       "      <td>171.049877</td>\n",
       "      <td>01:30</td>\n",
       "      <td>Bulawayo</td>\n",
       "      <td>BS</td>\n",
       "      <td>0.376326</td>\n",
       "      <td>0.332478</td>\n",
       "      <td>52.790485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15665</th>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.958602</td>\n",
       "      <td>0.081448</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>0.766612</td>\n",
       "      <td>96.096954</td>\n",
       "      <td>0.779548</td>\n",
       "      <td>96.118919</td>\n",
       "      <td>0.765727</td>\n",
       "      <td>...</td>\n",
       "      <td>293.358948</td>\n",
       "      <td>174.772258</td>\n",
       "      <td>174.849382</td>\n",
       "      <td>173.644722</td>\n",
       "      <td>01:30</td>\n",
       "      <td>Bulawayo</td>\n",
       "      <td>BS</td>\n",
       "      <td>0.459411</td>\n",
       "      <td>0.454320</td>\n",
       "      <td>6.831351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15666 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  month        a0  method_2_SUHI  quantile_a0  overall_evap_fract  \\\n",
       "0      2002      1  2.906066       0.760998     3.650000            0.757847   \n",
       "1      2002      2  2.596784       0.801647     2.140000            0.758275   \n",
       "2      2002      3       NaN       1.568914     2.869500            0.753038   \n",
       "3      2002      4  1.820394       0.734614     1.440000            0.793239   \n",
       "4      2002      5  1.585061       0.229486     0.220000            0.719868   \n",
       "...     ...    ...       ...            ...          ...                 ...   \n",
       "15661  2020      9  2.259935       1.288898     1.960000            0.113581   \n",
       "15662  2020     10  1.765050       0.766030     1.694000            0.233809   \n",
       "15663  2020     11  2.104820       0.983248     1.989333            0.346746   \n",
       "15664  2020     12  0.968285       0.238787     1.058500            0.773027   \n",
       "15665  2021      1 -1.958602       0.081448     1.080000            0.766612   \n",
       "\n",
       "       overall_rh  urban_evap_fract   urban_rh  rural_evap_fract  ...  \\\n",
       "0       95.929985          0.750305  95.864914          0.758065  ...   \n",
       "1       95.736839          0.752681  95.705627          0.758437  ...   \n",
       "2       96.402504          0.745333  96.382164          0.753261  ...   \n",
       "3       93.959358          0.781038  93.962883          0.793591  ...   \n",
       "4       84.684525          0.731550  84.930412          0.719531  ...   \n",
       "...           ...               ...        ...               ...  ...   \n",
       "15661   81.298790          0.101158  81.256645          0.114431  ...   \n",
       "15662   82.042282          0.232444  82.009880          0.233902  ...   \n",
       "15663   84.690918          0.351147  84.664955          0.346445  ...   \n",
       "15664   95.608086          0.787646  95.623184          0.772028  ...   \n",
       "15665   96.096954          0.779548  96.118919          0.765727  ...   \n",
       "\n",
       "        urban_t2m  overall_ssr   rural_ssr   urban_ssr  Overpass      City  \\\n",
       "0      291.251892   183.240585  183.293046  181.423597     10:30    Huambo   \n",
       "1      291.766907   191.752204  191.823445  189.284798     10:30    Huambo   \n",
       "2      291.673859   183.512472  183.582602  181.083538     10:30    Huambo   \n",
       "3      291.809235   198.372499  198.434762  196.216036     10:30    Huambo   \n",
       "4      291.129089   214.940667  215.010496  212.522158     10:30    Huambo   \n",
       "...           ...          ...         ...         ...       ...       ...   \n",
       "15661  292.991486   211.714841  211.779518  210.769278     01:30  Bulawayo   \n",
       "15662  295.421143   249.850531  249.928315  248.713337     01:30  Bulawayo   \n",
       "15663  296.661865   216.163103  216.242484  215.002569     01:30  Bulawayo   \n",
       "15664  293.581665   172.261565  172.344445  171.049877     01:30  Bulawayo   \n",
       "15665  293.358948   174.772258  174.849382  173.644722     01:30  Bulawayo   \n",
       "\n",
       "       Koppen_Geiger_2_Letter  rur_mean_evi  urb_mean_evi  \\\n",
       "0                          Cw           NaN           NaN   \n",
       "1                          Cw           NaN           NaN   \n",
       "2                          Cw           NaN           NaN   \n",
       "3                          Cw           NaN           NaN   \n",
       "4                          Cw           NaN           NaN   \n",
       "...                       ...           ...           ...   \n",
       "15661                      BS      0.142561      0.143414   \n",
       "15662                      BS      0.173692      0.176158   \n",
       "15663                      BS      0.255260      0.236684   \n",
       "15664                      BS      0.376326      0.332478   \n",
       "15665                      BS      0.459411      0.454320   \n",
       "\n",
       "       pixel_reliability_percent  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  \n",
       "...                          ...  \n",
       "15661                  97.407746  \n",
       "15662                  90.179933  \n",
       "15663                  55.504727  \n",
       "15664                  52.790485  \n",
       "15665                   6.831351  \n",
       "\n",
       "[15666 rows x 26 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_monthly_data_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
