{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import os\n",
    "import re  # regular expressions for getting lat lon grid\n",
    "import pathlib\n",
    "import warnings\n",
    "import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import rasterio as rio # for extracting subsets\n",
    "from rasterio.plot import plotting_extent #for plotting\n",
    "import earthpy as et\n",
    "import earthpy.plot as ep\n",
    "import earthpy.spatial as es\n",
    "import earthpy.mask as em\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "#for the reprojecting\n",
    "import pyproj\n",
    "from pyproj import CRS\n",
    "from pyproj import Transformer\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "from pyhdf.SD import SD, SDC\n",
    "import datetime as dt\n",
    "import scipy\n",
    "from scipy import interpolate\n",
    "import geopandas as gpd\n",
    "\n",
    "#for finding the mode\n",
    "from collections import Counter\n",
    "\n",
    "#load in csv of city lons and lats\n",
    "os.chdir(os.path.join(et.io.HOME, 'Documents', 'Python_Scripts', 'PROJECT'))\n",
    "\n",
    "CITY_COUNTRY_lat_lon = pd.read_excel('CITY_COUNTRY_lat_lon.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract the subdatasets of interest and return a dataframe \n",
    "#NDVI and EVI are vegetation indices\n",
    "#pixel reliability and VI quality are the quality checks\n",
    "\n",
    "def vi_dataframe_create(SATELLITE_NDVI, vi_file_name, city_top):\n",
    "    #set to directory with files\n",
    "    if city_top == True:\n",
    "        os.chdir(os.path.join('D:\\\\','MODIS_NDVI', CITY_COUNTRY, 'CITY_TOP', SATELLITE_NDVI))\n",
    "        #path to the file\n",
    "        data_path = os.path.join('D:\\\\','MODIS_NDVI', CITY_COUNTRY, 'CITY_TOP', SATELLITE_NDVI, vi_file_name)\n",
    "    else:        \n",
    "        os.chdir(os.path.join('D:\\\\','MODIS_NDVI', CITY_COUNTRY, SATELLITE_NDVI))\n",
    "        #path to the file\n",
    "        data_path = os.path.join('D:\\\\','MODIS_NDVI', CITY_COUNTRY, SATELLITE_NDVI, vi_file_name)\n",
    "    with rio.open(data_path) as dataset:\n",
    "    # Loop through each subdataset in HDF4 file\n",
    "        for name in dataset.subdatasets:\n",
    "        \n",
    "        # Use regular expression to identify if subdataset has NDVI in the name\n",
    "            if re.search(\"1 km monthly NDVI\", name):\n",
    "        \n",
    "                # Open the band subdataset\n",
    "                with rio.open(name) as subdataset:\n",
    "                    modis_meta = subdataset.profile\n",
    "                \n",
    "                    # Read band data as a 2 dim arr and append to list\n",
    "                    NDVI = subdataset.read(1)\n",
    "                \n",
    "            # Use regular expression to identify if subdataset has EVI in the name\n",
    "            if re.search(\"1 km monthly EVI\", name):\n",
    "        \n",
    "                # Open the band subdataset\n",
    "                with rio.open(name) as subdataset:\n",
    "                    modis_meta = subdataset.profile\n",
    "                \n",
    "                    # Read band data as a 2 dim arr and append to list\n",
    "                    EVI = subdataset.read(1)\n",
    "                \n",
    "       \n",
    "            # Use regular expression to identify if subdataset has reliability in the name (for pixel reliability)\n",
    "            if re.search(\"1 km monthly pixel reliability\", name):\n",
    "        \n",
    "                # Open the band subdataset\n",
    "                with rio.open(name) as subdataset:\n",
    "                    modis_meta = subdataset.profile\n",
    "                \n",
    "                    # Read band data as a 2 dim arr and append to list\n",
    "                    pixel_reliability = subdataset.read(1)\n",
    "                \n",
    "                \n",
    "              # Use regular expression to identify if subdataset has quality in the name (for VI Quality)\n",
    "            if re.search(\"1 km monthly VI Quality\", name):\n",
    "        \n",
    "                # Open the band subdataset\n",
    "                with rio.open(name) as subdataset:\n",
    "                    modis_meta = subdataset.profile\n",
    "                \n",
    "                    # Read band data as a 2 dim arr and append to list\n",
    "                    VI_quality = subdataset.read(1)       \n",
    "                \n",
    "                \n",
    "    #Create the coordinate grid\n",
    "    # Identify the data field- use the NDVI field but grid is same for all data\n",
    "    DATAFIELD_NAME = '1 km monthly EVI'\n",
    "\n",
    "    if SATELLITE_NDVI == 'MOD13A3':\n",
    "        GRID_NAME = 'MOD_Grid_monthly_1km_VI'\n",
    "    else:\n",
    "        GRID_NAME = 'MYD_Grid_monthly_1km_VI'\n",
    "        \n",
    "    hdf = SD(vi_file_name, SDC.READ)\n",
    "\n",
    "    # Read dataset.\n",
    "    data2D = hdf.select(DATAFIELD_NAME)\n",
    "    data = data2D[:,:].astype(np.float64)\n",
    "\n",
    "    # Read global attribute.\n",
    "    fattrs = hdf.attributes(full=1)\n",
    "    ga = fattrs[\"StructMetadata.0\"]\n",
    "    gridmeta = ga[0]\n",
    "\n",
    "    # Construct the grid.  Required information in global attribute called 'StructMetadata.0'\n",
    "\n",
    "    ul_regex = re.compile(r'''UpperLeftPointMtrs=\\(\n",
    "                                  (?P<upper_left_x>[+-]?\\d+\\.\\d+)\n",
    "                                  ,\n",
    "                                  (?P<upper_left_y>[+-]?\\d+\\.\\d+)\n",
    "                                  \\)''', re.VERBOSE)\n",
    "    match = ul_regex.search(gridmeta)\n",
    "    x0 = np.float(match.group('upper_left_x')) \n",
    "    y0 = np.float(match.group('upper_left_y')) \n",
    "\n",
    "    lr_regex = re.compile(r'''LowerRightMtrs=\\(\n",
    "                                  (?P<lower_right_x>[+-]?\\d+\\.\\d+)\n",
    "                                  ,\n",
    "                                  (?P<lower_right_y>[+-]?\\d+\\.\\d+)\n",
    "                                  \\)''', re.VERBOSE)\n",
    "    match = lr_regex.search(gridmeta)\n",
    "    x1 = np.float(match.group('lower_right_x')) \n",
    "    y1 = np.float(match.group('lower_right_y')) \n",
    "    ny, nx = data.shape\n",
    "    xinc = (x1 - x0) / nx\n",
    "    yinc = (y1 - y0) / ny\n",
    "\n",
    "    x = np.linspace(x0, x0 + xinc*nx, nx)\n",
    "    y = np.linspace(y0, y0 + yinc*ny, ny)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "    # convert the grid back to lat/lons.\n",
    "    transformer = Transformer.from_crs(\"+proj=sinu +R=6371007.181 +nadgrids=@null +wktext\", \"EPSG:4326\")\n",
    "    lat, lon = transformer.transform(xv, yv)\n",
    "\n",
    "    #Apply scale factors\n",
    "    scale_factor_NDVI = 0.0001\n",
    "    scale_factor_EVI = 0.0001\n",
    "\n",
    "    NDVI = NDVI*scale_factor_NDVI\n",
    "    EVI = EVI*scale_factor_EVI\n",
    "\n",
    "    #Create the lists to be combined to create a dataframe\n",
    "    NDVI_list = NDVI.flatten()\n",
    "    EVI_list = EVI.flatten()\n",
    "    pixel_reliability_list = pixel_reliability.flatten()\n",
    "    VI_quality_list = VI_quality.flatten()\n",
    "    Lon_list = lon.flatten()\n",
    "    Lat_list = lat.flatten()\n",
    "\n",
    "    #Create the dataframe\n",
    "\n",
    "    df = pd.DataFrame(list(zip(NDVI_list, EVI_list, pixel_reliability_list, VI_quality_list, Lon_list, Lat_list)), \n",
    "               columns =['NDVI', 'EVI','pixel_reliability', 'VI_quality','Longitude', 'Latitude']) \n",
    "\n",
    "    #Create dataframe of the required area\n",
    "    df_subset = df[(df.Latitude >= min_lat) & (df.Latitude <= max_lat) & (df.Longitude >= min_lon) & (df.Longitude <= max_lon)]\n",
    "    #sort by lat, lon\n",
    "    df_subset = df_subset.sort_values(by=['Latitude', 'Longitude'])\n",
    "    \n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the files that go over 2 images (Bulawayo)/ or remove files already been run for \n",
    "#CITY_COUNTRY_lat_lon = CITY_COUNTRY_lat_lon[18:].reset_index(drop = 'True')\n",
    "\n",
    "#CITY_COUNTRY_lat_lon = CITY_COUNTRY_lat_lon.iloc[[25,36,37]].reset_index(drop = 'True')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\project_environment\\lib\\site-packages\\rasterio\\__init__.py:221: NotGeoreferencedWarning: Dataset has no geotransform set. The identity matrix may be returned.\n",
      "  s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#create a list of the urban mean evi and the rural mean evi\n",
    "#runtime start:11.39\n",
    "for m in range(len(CITY_COUNTRY_lat_lon)):\n",
    "\n",
    "    CITY_COUNTRY = CITY_COUNTRY_lat_lon.CITY_COUNTRY[m]\n",
    "    #Area to look at \n",
    "    min_lat = CITY_COUNTRY_lat_lon[CITY_COUNTRY_lat_lon['CITY_COUNTRY'] == CITY_COUNTRY]['min_lat'].values[0]\n",
    "    max_lat = CITY_COUNTRY_lat_lon[CITY_COUNTRY_lat_lon['CITY_COUNTRY'] == CITY_COUNTRY]['max_lat'].values[0]\n",
    "    min_lon = CITY_COUNTRY_lat_lon[CITY_COUNTRY_lat_lon['CITY_COUNTRY'] == CITY_COUNTRY]['min_lon'].values[0]\n",
    "    max_lon = CITY_COUNTRY_lat_lon[CITY_COUNTRY_lat_lon['CITY_COUNTRY'] == CITY_COUNTRY]['max_lon'].values[0]\n",
    "    \n",
    "    #load in the data with the list of the city vi files\n",
    "    #get lists of all the VI files and their months/ years\n",
    "    #TERRA\n",
    "    SATELLITE_NDVI = 'MOD13A3'\n",
    "    os.chdir(os.path.join('D:\\\\','MODIS_NDVI', CITY_COUNTRY, SATELLITE_NDVI))\n",
    "\n",
    "    file_list = []\n",
    "    month_list = []\n",
    "    year_list = []\n",
    "    for filename in os.listdir():\n",
    "        if filename.endswith(\".hdf\"): \n",
    "            file_list.append(filename)\n",
    "            yeardoy = filename.split('.')[1][1:]\n",
    "            month_list.append(dt.datetime.strptime(yeardoy, '%Y%j').strftime('%m'))\n",
    "            year_list.append(dt.datetime.strptime(yeardoy, '%Y%j').strftime('%Y'))\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    d = {'Filename': file_list, 'Month': month_list, 'Year': year_list}\n",
    "    vi_look_up_terra = pd.DataFrame(data = d)\n",
    "    vi_look_up_terra = vi_look_up_terra[vi_look_up_terra['Year'] != '2021'] #remove 2021 from the dataframe (not using)\n",
    "    #AQUA\n",
    "    SATELLITE_NDVI = 'MYD13A3'\n",
    "    os.chdir(os.path.join('D:\\\\','MODIS_NDVI', CITY_COUNTRY, SATELLITE_NDVI))\n",
    "\n",
    "    file_list = []\n",
    "    month_list = []\n",
    "    year_list = []\n",
    "    for filename in os.listdir():\n",
    "        if filename.endswith(\".hdf\"): \n",
    "            file_list.append(filename)\n",
    "            yeardoy = filename.split('.')[1][1:]\n",
    "            month_list.append(dt.datetime.strptime(yeardoy, '%Y%j').strftime('%m'))\n",
    "            year_list.append(dt.datetime.strptime(yeardoy, '%Y%j').strftime('%Y'))\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    d = {'Filename': file_list, 'Month': month_list, 'Year': year_list}\n",
    "    vi_look_up_aqua = pd.DataFrame(data = d) \n",
    "    vi_look_up_aqua = vi_look_up_aqua[vi_look_up_aqua['Year'] != '2021'] #remove 2021 from the dataframe (not using)\n",
    "    #Load in LULC data, these bases were created in JASMIN\n",
    "    os.chdir(os.path.join(et.io.HOME, 'Documents', 'Python_Scripts', 'PROJECT'))\n",
    "    with open('UHI_Project_Pickle_Files\\LULC_Pickles\\Crop_wbuffer_LULC\\LULC_{}.pkl'.format(CITY_COUNTRY), 'rb') as f:\n",
    "         LULC_df = pickle.load(f)\n",
    "    #make sure LULC sorted by latitude and longitude\n",
    "    LULC_df = LULC_df.sort_values(by=['Latitude', 'Longitude'])   \n",
    "\n",
    "    #create the df to be filled with the mean values\n",
    "    vi_means_df = vi_look_up_aqua.copy()\n",
    "    vi_means_df = vi_means_df.rename(columns={\"Filename\": \"Aqua_Filename\"})\n",
    "    vi_means_df['Terra_Filename'] = np.nan\n",
    "    vi_means_df['rur_mean_evi'] = np.nan\n",
    "    vi_means_df['urb_mean_evi'] = np.nan\n",
    "    vi_means_df['pixel_reliability_percent'] = np.nan        \n",
    "        \n",
    "    for n in range(len(vi_means_df)):     \n",
    "        #add the evi to the base\n",
    "        #create the aqua and terra dataframes\n",
    "        aqua_vi_file_name = vi_look_up_aqua.Filename[n]\n",
    "        aqua_mon =  vi_look_up_aqua.Month[n]\n",
    "        aqua_year = vi_look_up_aqua.Year[n]\n",
    "        aqua_vi_df = vi_dataframe_create('MYD13A3', aqua_vi_file_name, city_top = False)\n",
    "\n",
    "        terra_vi_file_name = vi_look_up_terra[(vi_look_up_terra.Month == aqua_mon) & (vi_look_up_terra.Year == aqua_year)].Filename.values[0]\n",
    "        terra_vi_df = vi_dataframe_create('MOD13A3', terra_vi_file_name, city_top = False)\n",
    "\n",
    "        #create a df containing final evi values (if aqua not reliable, use terra)\n",
    "        LULC_df2 = LULC_df.copy()\n",
    "        LULC_df2['aqua_evi'] = aqua_vi_df.EVI.values\n",
    "        LULC_df2['aqua_pixel_reliability'] = aqua_vi_df.pixel_reliability.values\n",
    "        LULC_df2['terra_evi'] = terra_vi_df.EVI.values\n",
    "        LULC_df2['terra_pixel_reliability'] = terra_vi_df.pixel_reliability.values\n",
    "        LULC_df2['evi_final'] = aqua_vi_df.EVI.values\n",
    "        LULC_df2.loc[((LULC_df2['aqua_pixel_reliability'] == 1) & (LULC_df2['terra_pixel_reliability'] == 0)\n",
    "                 ), 'evi_final'] = LULC_df2['terra_evi']\n",
    "        LULC_df2['pixel_reliablity_final'] = 0\n",
    "        LULC_df2.loc[((LULC_df2['aqua_pixel_reliability'] == 1) & (LULC_df2['terra_pixel_reliability'] == 1)\n",
    "                 ), 'pixel_reliablity_final'] = 1\n",
    "\n",
    "        #calculate the average rur/ urb evi and pixel reliability percent\n",
    "        rur_mean_evi = LULC_df2[(LULC_df2['pixel_reliablity_final'] == 0)&(LULC_df2['lccs_class_overall_2015'] != 190\n",
    "                                                                      )].evi_final.mean()\n",
    "        urb_mean_evi = LULC_df2[(LULC_df2['pixel_reliablity_final'] == 0)&(LULC_df2['lccs_class_overall_2015'] == 190\n",
    "                                                                      )].evi_final.mean()\n",
    "        pixel_reliability_percent = 100* len(LULC_df2[(LULC_df2['pixel_reliablity_final'] == 0)])/ len(LULC_df2)\n",
    "\n",
    "        #add to the overall dataframe with the list of files and the means\n",
    "        vi_means_df.loc[n,'Terra_Filename'] = terra_vi_file_name\n",
    "        vi_means_df.loc[n,'rur_mean_evi'] = rur_mean_evi\n",
    "        vi_means_df.loc[n,'urb_mean_evi'] = urb_mean_evi\n",
    "        vi_means_df.loc[n,'pixel_reliability_percent'] = pixel_reliability_percent\n",
    "\n",
    "    #save the df\n",
    "    os.chdir(os.path.join('D:\\\\','MODIS_NDVI', CITY_COUNTRY))\n",
    "    pickle_name = 'vi_means_df_{}.pkl'.format(CITY_COUNTRY)\n",
    "    with open(pickle_name, 'wb') as f:\n",
    "        pickle.dump(vi_means_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
